ğŸ›’ **Real-Time Retail Streaming Pipeline**

âœ¨ **Project Overview**


A real-time retail data pipeline demonstrating Bronze â†’ Silver â†’ Gold architecture using PySpark Structured Streaming on Azure Databricks.


ğŸ’¡ **Key features:**


Stream CSV data uploaded to DBFS input folder into Delta Bronze tables

Clean & deduplicate in Silver

Aggregate key business metrics in Gold (total sales, quantity, unique customers, average order value)

Visualize KPIs in Databricks for each country

Data Source: UCI Online Retail Dataset (https://archive.ics.uci.edu/ml/datasets/Online+Retail)

âš™ï¸**How to Run**


1ï¸âƒ£ Upload CSVs to the DBFS input folder (/FileStore/tables).

2ï¸âƒ£ Run Bronze Notebook â†’ raw ingestion to Delta Bronze.

3ï¸âƒ£ Run Silver Notebook â†’ clean & deduplicate data â†’ Delta Silver.

4ï¸âƒ£ Run Gold Notebook â†’ perform aggregations â†’ Delta Gold.

5ï¸âƒ£ Visualize KPIs in Databricks using built-in charts for:

  Total Sales
  
  Average Order Value
  
  Unique Customers
  (All at a country level)

**Example: read Gold table**

        gold_df = spark.read.format("delta").load("dbfs:/mnt/gold/events")
        gold_df.display()


ğŸ“Š **Key Metrics**


Total Sales -->	Sum of (Quantity Ã— UnitPrice) per country

Total Quantity --> Sum of Quantity per country

Unique Customers --> Approx. count of customers per country

Avg Order Value --> Total Sales Ã· Total Quantity per country


ğŸ›  **Technologies Used**


1.Apache Spark / PySpark â€“ Stream processing

2.Delta Lake â€“ ACID-compliant storage

3.Azure Databricks â€“ Development & execution

4.Python â€“ Core programming

5.Databricks Visualizations â€“ Charts & graphs

6.DBFS Storage / CSV Input â€“ Source data


ğŸ”**Monitoring & Debugging**


a)Streaming checkpoints ensure fault tolerance

b)Check active streams:

spark.streams.active


c)Deduplication, type casting, and approximate distinct counts handled in Silver & Gold


ğŸš€**Future Enhancements**


1.Integrate Power BI / Synapse Analytics dashboards

2.EventHub triggers for fully real-time ingestion

3.Optional ADF orchestration for hybrid workflows

4.Log Analytics Monitoring for streaming metrics


ğŸ¯**Skills Demonstrated**


a)Structured streaming with PySpark

b)Bronzeâ€“Silverâ€“Gold Delta Lake architecture

c)Deduplication & data cleaning

d)Real-time KPI computation

e)Databricks visualizations for country-level metrics
